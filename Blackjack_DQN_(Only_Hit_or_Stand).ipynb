{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCMcUqmLnoIf",
        "outputId": "e8b2515b-56ab-4c2d-9a1c-a8206cecbb3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.error import DependencyNotInstalled\n",
        "\n",
        "\n",
        "def cmp(a, b):\n",
        "    return float(a > b) - float(a < b)\n",
        "\n",
        "\n",
        "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
        "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
        "\n",
        "\n",
        "def draw_card(np_random):\n",
        "    return int(np_random.choice(deck))\n",
        "\n",
        "\n",
        "def draw_hand(np_random):\n",
        "    return [draw_card(np_random), draw_card(np_random)]\n",
        "\n",
        "\n",
        "def usable_ace(hand):  # Does this hand have a usable ace?\n",
        "    return 1 in hand and sum(hand) + 10 <= 21\n",
        "\n",
        "\n",
        "def sum_hand(hand):  # Return current hand total\n",
        "    if usable_ace(hand):\n",
        "        return sum(hand) + 10\n",
        "    return sum(hand)\n",
        "\n",
        "\n",
        "def is_bust(hand):  # Is this hand a bust?\n",
        "    return sum_hand(hand) > 21\n",
        "\n",
        "\n",
        "def score(hand):  # What is the score of this hand (0 if bust)\n",
        "    return 0 if is_bust(hand) else sum_hand(hand)\n",
        "\n",
        "\n",
        "def is_natural(hand):  # Is this hand a natural blackjack?\n",
        "    return sorted(hand) == [1, 10]\n",
        "\n",
        "\n",
        "class BlackjackEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Blackjack is a card game where the goal is to beat the dealer by obtaining cards\n",
        "    that sum to closer to 21 (without going over 21) than the dealers cards.\n",
        "    ## Description\n",
        "    The game starts with the dealer having one face up and one face down card,\n",
        "    while the player has two face up cards. All cards are drawn from an infinite deck\n",
        "    (i.e. with replacement).\n",
        "    The card values are:\n",
        "    - Face cards (Jack, Queen, King) have a point value of 10.\n",
        "    - Aces can either count as 11 (called a 'usable ace') or 1.\n",
        "    - Numerical cards (2-9) have a value equal to their number.\n",
        "    The player has the sum of cards held. The player can request\n",
        "    additional cards (hit) until they decide to stop (stick) or exceed 21 (bust,\n",
        "    immediate loss).\n",
        "    After the player sticks, the dealer reveals their facedown card, and draws cards\n",
        "    until their sum is 17 or greater. If the dealer goes bust, the player wins.\n",
        "    If neither the player nor the dealer busts, the outcome (win, lose, draw) is\n",
        "    decided by whose sum is closer to 21.\n",
        "    This environment corresponds to the version of the blackjack problem\n",
        "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
        "    by Sutton and Barto [<a href=\"#blackjack_ref\">1</a>].\n",
        "    ## Action Space\n",
        "    The action shape is `(1,)` in the range `{0, 1}` indicating\n",
        "    whether to stick or hit.\n",
        "    - 0: Stick\n",
        "    - 1: Hit\n",
        "    ## Observation Space\n",
        "    The observation consists of a 3-tuple containing: the player's current sum,\n",
        "    the value of the dealer's one showing card (1-10 where 1 is ace),\n",
        "    and whether the player holds a usable ace (0 or 1).\n",
        "    The observation is returned as `(int(), int(), int())`.\n",
        "    ## Starting State\n",
        "    The starting state is initialised in the following range.\n",
        "    | Observation               | Min  | Max  |\n",
        "    |---------------------------|------|------|\n",
        "    | Player current sum        |  4   |  12  |\n",
        "    | Dealer showing card value |  2   |  11  |\n",
        "    | Usable Ace                |  0   |  1   |\n",
        "    ## Rewards\n",
        "    - win game: +1\n",
        "    - lose game: -1\n",
        "    - draw game: 0\n",
        "    - win game with natural blackjack:\n",
        "    +1.5 (if <a href=\"#nat\">natural</a> is True)\n",
        "    +1 (if <a href=\"#nat\">natural</a> is False)\n",
        "    ## Episode End\n",
        "    The episode ends if the following happens:\n",
        "    - Termination:\n",
        "    1. The player hits and the sum of hand exceeds 21.\n",
        "    2. The player sticks.\n",
        "    An ace will always be counted as usable (11) unless it busts the player.\n",
        "    ## Information\n",
        "    No additional information is returned.\n",
        "    ## Arguments\n",
        "    ```python\n",
        "    import gymnasium as gym\n",
        "    gym.make('Blackjack-v1', natural=False, sab=False)\n",
        "    ```\n",
        "    <a id=\"nat\"></a>`natural=False`: Whether to give an additional reward for\n",
        "    starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21).\n",
        "    <a id=\"sab\"></a>`sab=False`: Whether to follow the exact rules outlined in the book by\n",
        "    Sutton and Barto. If `sab` is `True`, the keyword argument `natural` will be ignored.\n",
        "    If the player achieves a natural blackjack and the dealer does not, the player\n",
        "    will win (i.e. get a reward of +1). The reverse rule does not apply.\n",
        "    If both the player and the dealer get a natural, it will be a draw (i.e. reward 0).\n",
        "    ## References\n",
        "    <a id=\"blackjack_ref\"></a>[1] R. Sutton and A. Barto, “Reinforcement Learning:\n",
        "    An Introduction” 2020. [Online]. Available: [http://www.incompleteideas.net/book/RLbook2020.pdf](http://www.incompleteideas.net/book/RLbook2020.pdf)\n",
        "    ## Version History\n",
        "    * v1: Fix the natural handling in Blackjack\n",
        "    * v0: Initial version release\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode: Optional[str] = None, natural=False, sab=False):\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Tuple(\n",
        "            (spaces.Discrete(32), spaces.Discrete(11), spaces.Discrete(2))\n",
        "        )\n",
        "\n",
        "        # Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\n",
        "        # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n",
        "        self.natural = natural\n",
        "\n",
        "        # Flag for full agreement with the (Sutton and Barto, 2018) definition. Overrides self.natural\n",
        "        self.sab = sab\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "        if action:  # hit: add a card to players hand and return\n",
        "            self.player.append(draw_card(self.np_random))\n",
        "            if is_bust(self.player):\n",
        "                terminated = True\n",
        "                reward = -1.0\n",
        "            else:\n",
        "                terminated = False\n",
        "                reward = 0.0\n",
        "        else:  # stick: play out the dealers hand, and score\n",
        "            terminated = True\n",
        "            while sum_hand(self.dealer) < 17:\n",
        "                self.dealer.append(draw_card(self.np_random))\n",
        "            reward = cmp(score(self.player), score(self.dealer))\n",
        "            if self.sab and is_natural(self.player) and not is_natural(self.dealer):\n",
        "                # Player automatically wins. Rules consistent with S&B\n",
        "                reward = 1.0\n",
        "            elif (\n",
        "                not self.sab\n",
        "                and self.natural\n",
        "                and is_natural(self.player)\n",
        "                and reward == 1.0\n",
        "            ):\n",
        "                # Natural gives extra points, but doesn't autowin. Legacy implementation\n",
        "                reward = 1.5\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self._get_obs(), reward, terminated, False, {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player))\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        self.dealer = draw_hand(self.np_random)\n",
        "        self.player = draw_hand(self.np_random)\n",
        "\n",
        "        _, dealer_card_value, _ = self._get_obs()\n",
        "\n",
        "        suits = [\"C\", \"D\", \"H\", \"S\"]\n",
        "        self.dealer_top_card_suit = self.np_random.choice(suits)\n",
        "\n",
        "        if dealer_card_value == 1:\n",
        "            self.dealer_top_card_value_str = \"A\"\n",
        "        elif dealer_card_value == 10:\n",
        "            self.dealer_top_card_value_str = self.np_random.choice([\"J\", \"Q\", \"K\"])\n",
        "        else:\n",
        "            self.dealer_top_card_value_str = str(dealer_card_value)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            import pygame\n",
        "        except ImportError as e:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"pygame is not installed, run `pip install gymnasium[toy-text]`\"\n",
        "            ) from e\n",
        "\n",
        "        player_sum, dealer_card_value, usable_ace = self._get_obs()\n",
        "        screen_width, screen_height = 600, 500\n",
        "        card_img_height = screen_height // 3\n",
        "        card_img_width = int(card_img_height * 142 / 197)\n",
        "        spacing = screen_height // 20\n",
        "\n",
        "        bg_color = (7, 99, 36)\n",
        "        white = (255, 255, 255)\n",
        "\n",
        "        if not hasattr(self, \"screen\"):\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.init()\n",
        "                self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "            else:\n",
        "                pygame.font.init()\n",
        "                self.screen = pygame.Surface((screen_width, screen_height))\n",
        "\n",
        "        if not hasattr(self, \"clock\"):\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        self.screen.fill(bg_color)\n",
        "\n",
        "        def get_image(path):\n",
        "            cwd = os.path.dirname(__file__)\n",
        "            image = pygame.image.load(os.path.join(cwd, path))\n",
        "            return image\n",
        "\n",
        "        def get_font(path, size):\n",
        "            cwd = os.path.dirname(__file__)\n",
        "            font = pygame.font.Font(os.path.join(cwd, path), size)\n",
        "            return font\n",
        "\n",
        "        small_font = get_font(\n",
        "            os.path.join(\"font\", \"Minecraft.ttf\"), screen_height // 15\n",
        "        )\n",
        "        dealer_text = small_font.render(\n",
        "            \"Dealer: \" + str(dealer_card_value), True, white\n",
        "        )\n",
        "        dealer_text_rect = self.screen.blit(dealer_text, (spacing, spacing))\n",
        "\n",
        "        def scale_card_img(card_img):\n",
        "            return pygame.transform.scale(card_img, (card_img_width, card_img_height))\n",
        "\n",
        "        dealer_card_img = scale_card_img(\n",
        "            get_image(\n",
        "                os.path.join(\n",
        "                    \"img\",\n",
        "                    f\"{self.dealer_top_card_suit}{self.dealer_top_card_value_str}.png\",\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        dealer_card_rect = self.screen.blit(\n",
        "            dealer_card_img,\n",
        "            (\n",
        "                screen_width // 2 - card_img_width - spacing // 2,\n",
        "                dealer_text_rect.bottom + spacing,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        hidden_card_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "        self.screen.blit(\n",
        "            hidden_card_img,\n",
        "            (\n",
        "                screen_width // 2 + spacing // 2,\n",
        "                dealer_text_rect.bottom + spacing,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        player_text = small_font.render(\"Player\", True, white)\n",
        "        player_text_rect = self.screen.blit(\n",
        "            player_text, (spacing, dealer_card_rect.bottom + 1.5 * spacing)\n",
        "        )\n",
        "\n",
        "        large_font = get_font(os.path.join(\"font\", \"Minecraft.ttf\"), screen_height // 6)\n",
        "        player_sum_text = large_font.render(str(player_sum), True, white)\n",
        "        player_sum_text_rect = self.screen.blit(\n",
        "            player_sum_text,\n",
        "            (\n",
        "                screen_width // 2 - player_sum_text.get_width() // 2,\n",
        "                player_text_rect.bottom + spacing,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if usable_ace:\n",
        "            usable_ace_text = small_font.render(\"usable ace\", True, white)\n",
        "            self.screen.blit(\n",
        "                usable_ace_text,\n",
        "                (\n",
        "                    screen_width // 2 - usable_ace_text.get_width() // 2,\n",
        "                    player_sum_text_rect.bottom + spacing // 2,\n",
        "                ),\n",
        "            )\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if hasattr(self, \"screen\"):\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "\n",
        "\n",
        "# Pixel art from Mariia Khmelnytska (https://www.123rf.com/photo_104453049_stock-vector-pixel-art-playing-cards-standart-deck-vector-set.html)"
      ],
      "metadata": {
        "id": "-Eb8xsctZpdc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8WUD75m5Sbjq"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "class DQNAgent():\n",
        "    def __init__(self, env, epsilon=1.0, alpha=0.5, gamma=0.9, time = 30000):\n",
        "        self.env = env\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.state_size = env.observation_space\n",
        "        self.memory = deque(maxlen=2000) # Record past experiences- [(state, action, reward, next_state, done)...]\n",
        "        self.epsilon = epsilon   # Random exploration factor\n",
        "        self.alpha = alpha       # Learning factor\n",
        "        self.gamma = gamma       # Discount factor- closer to 1 learns well into distant future\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning = True\n",
        "        self.model = self._build_model()\n",
        "        \n",
        "        self.time = time \n",
        "        self.time_left = time # Epsilon Decay\n",
        "        self.small_decrement = (0.4 * epsilon) / (0.3 * self.time_left) # reduce epsilon\n",
        "        print('HELLO')\n",
        "    \n",
        "    # Build Neural Net\n",
        "    def _build_model(self):\n",
        "#         print(type(self.state_size))\n",
        "        model = Sequential()\n",
        "        model.add(Dense(32, input_shape = (2,), kernel_initializer='random_uniform', activation='relu'))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='softmax'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=self.alpha))\n",
        "        \n",
        "        return model\n",
        "       \n",
        "#     # Remember function that stores states, actions, rewards, and done to memory\n",
        "#     def remember(self, state, action, reward, next_state, done):\n",
        "#         self.memory.append([state, action, reward, next_state, done])\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Choose which action to take, based on the observation. \n",
        "        Uses greedy epsilon for exploration/exploitation.\n",
        "        \"\"\"\n",
        "\n",
        "        # if random number > epsilon, act 'rationally'. otherwise, choose random action\n",
        "        \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "#             print(np.random.rand())\n",
        "#             print(self.epsilon)\n",
        "#             print('random')\n",
        "#             print('-------')\n",
        "            \n",
        "            action = random.randrange(self.action_size)\n",
        "#             print('random: ' + str(action))\n",
        "\n",
        "            \n",
        "        else:\n",
        "#             print('logic')\n",
        "\n",
        "            action_value = self.model.predict(state)\n",
        "#             print(action_value)\n",
        "#             print(action_value)\n",
        "#             print(action_value)\n",
        "#             print('-------')\n",
        "            action = np.argmax(action_value[0])\n",
        "#             print(action)\n",
        "        \n",
        "        self.update_parameters()\n",
        "        return action\n",
        "        \n",
        "    def update_parameters(self):\n",
        "        \"\"\"\n",
        "        Update epsilon and alpha after each action\n",
        "        Set them to 0 if not learning\n",
        "        \"\"\"\n",
        "#         print(self.time_left)\n",
        "        if self.time_left > 0.9 * self.time:\n",
        "            self.epsilon -= self.small_decrement\n",
        "        elif self.time_left > 0.7 * self.time:\n",
        "            self.epsilon -= self.small_decrement\n",
        "        elif self.time_left > 0.5 * self.time:\n",
        "            self.epsilon -= self.small_decrement\n",
        "#             print('0.5')\n",
        "        elif self.time_left > 0.3 * self.time:\n",
        "#             print('0.2')\n",
        "            self.epsilon -= self.small_decrement\n",
        "        elif self.time_left > 0.1 * self.time:\n",
        "            self.epsilon -= self.small_decrement\n",
        "#         elif self.time_left < 0.05 * self.time:\n",
        "#             self.epsilon = 0.000\n",
        "#             self.learning = False\n",
        "\n",
        "#         print(self.time_left)\n",
        "        #         print(self.time)\n",
        "        self.time_left -= 1       \n",
        "\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \n",
        "        \n",
        "        \n",
        "#         minibatch = random.sample(self.memory, batch_size)\n",
        "#         print(minibatch)\n",
        "        \n",
        "        target = reward\n",
        "#         print('STATE: ' + str(state))\n",
        "#             print('next_state: ' + str(state))\n",
        "\n",
        "#         print('target: ' + str(target))\n",
        "\n",
        "        if not done:\n",
        "            target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "\n",
        "#         print('target: ' + str(target))\n",
        "\n",
        "\n",
        "#         print('action: ' + str(action))\n",
        "#             print(self.model.predict(next_state))\n",
        "#         print(np.amax(self.model.predict(next_state)[0]))\n",
        "\n",
        "        target_f = self.model.predict(state)\n",
        "#         print(target_f)\n",
        "#         print('target_f: ' + str(target_f))\n",
        "\n",
        "\n",
        "        target_f[0][action] = target\n",
        "#         print('target_f: ' + str(target_f))\n",
        "#             print('target_f: ' + str(target_f))\n",
        "#         print('-------')\n",
        "\n",
        "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        \n",
        "        \n",
        "#             print(self.time)\n",
        "#         print(self.epsilon)\n",
        "\n",
        "    def get_optimal_strategy(self):\n",
        "        index = []\n",
        "        for x in range(0,21):\n",
        "            for y in range(1,11):\n",
        "                index.append((x,y))\n",
        "\n",
        "        df = pd.DataFrame(index = index, columns = ['Stand', 'Hit'])\n",
        "\n",
        "        for ind in range(0,21):\n",
        "            outcome = self.model.predict([np.array([index[ind]])], batch_size=1)\n",
        "            print(outcome)\n",
        "            print(type(outcome[0][0]))\n",
        "            print(outcome[0][1])\n",
        "            df.loc[ind, 'Stand'] = outcome[0][0]\n",
        "            df.loc[ind, 'Hit'] = outcome[0][1]\n",
        "\n",
        "\n",
        "        df['Optimal'] = df.apply(lambda x : 'Hit' if x['Hit'] >= x['Stand'] else 'Stand', axis=1)\n",
        "        df.to_csv('optimal_policy.csv')\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cTeHFSuSbjr",
        "outputId": "ad74d93d-5af6-4f62-f49e-f0f5889b2a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO\n",
            "1/1 [==============================] - 0s 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        }
      ],
      "source": [
        "env = BlackjackEnv()\n",
        "env.reset()\n",
        "# env = wrappers.Monitor(env, './logs/blackjack-Q', False, True)\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# print(agent.model)\n",
        "\n",
        "num_rounds = 100 # Payout calculated over num_rounds\n",
        "num_samples = 50 # num_rounds simulated over num_samples\n",
        "\n",
        "agent = DQNAgent(env=env, epsilon=1.0, alpha=0.001, gamma=0.1, time=7500)\n",
        "\n",
        "average_payouts = []\n",
        "\n",
        "state,_ = env.reset()\n",
        "state = np.reshape(state[0:2], [1,2])\n",
        "for sample in range(num_samples):\n",
        "    round = 1\n",
        "    total_payout = 0 # store total payout per sample\n",
        "    while round <= num_rounds:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, payout, done, _, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state[0:2], [1,2])\n",
        "\n",
        "        \n",
        "        total_payout += payout    \n",
        "#         if agent.learning:\n",
        "        agent.learn(state, action, payout, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        state = np.reshape(state[0:2], [1,2])\n",
        "        \n",
        "        if done:\n",
        "            state,_ = env.reset() # Environment deals new cards to player and dealer\n",
        "            state = np.reshape(state[0:2], [1,2])\n",
        "            round += 1\n",
        "\n",
        "    average_payouts.append(total_payout)\n",
        "\n",
        "    if sample % 10 == 0:\n",
        "        print('Done with sample: ' + str(sample) + str(\"   --- %s seconds ---\" % (time.time() - start_time)))\n",
        "        print(agent.epsilon)\n",
        "\n",
        "print(agent.get_optimal_strategy())\n",
        "\n",
        "# Plot payout per 1000 episodes for each value of 'sample'\n",
        "\n",
        "plt.plot(average_payouts)           \n",
        "plt.xlabel('num_samples')\n",
        "plt.ylabel('payout after 1000 rounds')\n",
        "plt.show()      \n",
        "    \n",
        "print (\"Average payout after {} rounds is {}\".format(num_rounds, sum(average_payouts)/(num_samples)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2_6j1LcSbjr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ef9_6N88Sbjr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}